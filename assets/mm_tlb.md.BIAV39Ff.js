import{_ as a,c as i,o as s,aj as t}from"./chunks/framework.CcbH9oJh.js";const _=JSON.parse('{"title":"TLB","description":"","frontmatter":{"head":[["meta",{"property":"og:title","content":"TLB | Blog"}]]},"headers":[],"relativePath":"mm/tlb.md","filePath":"mm/tlb.md","lastUpdated":1761828946000}'),l={name:"mm/tlb.md"};function n(o,e,p,r,d,h){return s(),i("div",null,e[0]||(e[0]=[t(`<h1 id="tlb" tabindex="-1">TLB <a class="header-anchor" href="#tlb" aria-label="Permalink to “TLB”">​</a></h1><p><code>mm/mmu_gather.c</code></p><h2 id="使用案例" tabindex="-1">使用案例 <a class="header-anchor" href="#使用案例" aria-label="Permalink to “使用案例”">​</a></h2><p>对于 kmalloc 区域的，直接 free_pages 即可，不需要修改页表。 对于 vmalloc 区域的，以及用户页面，典型的就是私有匿名映射页面，在释放前需要改页表的，就需要用到这些 API 了。</p><p>一般的用法流程是：</p><ol><li><code>tlb_gather_mmu()</code> 初始化 <code>struct mmu_gather</code></li><li><code>unmap_page_range()</code> 修改页表，此时还未 TLB flush</li><li><code>tlb_finish_mmu()</code> 进行 TLB invalidate 并将页面释放回 SLUB</li></ol><p>案例：</p><ul><li><a href="./oom"><code>__oom_reap_task_mm()</code></a> 释放用户进程的私有匿名映射页面</li><li><code>exit_mmap()</code></li></ul><h2 id="部分代码分析" tabindex="-1">部分代码分析 <a class="header-anchor" href="#部分代码分析" aria-label="Permalink to “部分代码分析”">​</a></h2><div class="language-cpp line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">tlb_finish_mmu</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">  tlb_flush_mmu</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">    tlb_flush_mmu_tlbonly</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tlb)</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> /* TLB invalidate */</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">    tlb_flush_mmu_free</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">      tlb_table_flush</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> /* TODO */</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">      tlb_batch_pages_flush</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> /* 释放和 swap cache */</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">        __tlb_batch_free_encoded_pages</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()-&gt;</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">free_pages_and_swap_cache</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">          free_swap_cache</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">          folios_put_refs</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()-&gt;</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">free_unref_folios</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()-&gt;</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">free_one_page</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  /* 这里释放的是 struct mmu_gather_batch */</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">  tlb_batch_list_free</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()-&gt;</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">free_pages</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><h2 id="文档" tabindex="-1">文档 <a class="header-anchor" href="#文档" aria-label="Permalink to “文档”">​</a></h2><p>来自 <code>include/asm-generic/tlb.h</code> 的注释。</p><p>Generic MMU-gather implementation.</p><p>The <code>mmu_gather</code> data structure is used by the mm code to implement the correct and efficient ordering of freeing pages and TLB invalidations.</p><p>This correct ordering is:</p><ol><li>unhook page</li><li>TLB invalidate page</li><li>free page</li></ol><p>That is, we must never free a page before we have ensured there are no live translations left to it. Otherwise it might be possible to observe (or worse, change) the page content after it has been reused.</p><p>The mmu_gather API consists of:</p><ul><li><p><code>tlb_gather_mmu() / tlb_gather_mmu_fullmm() / tlb_finish_mmu()</code></p><p>start and finish a mmu_gather</p><p>Finish in particular will issue a (final) TLB invalidate and free all (remaining) queued pages.</p></li><li><p><code>tlb_start_vma() / tlb_end_vma()</code>; marks the start / end of a VMA</p><p>Defaults to flushing at <code>tlb_end_vma()</code> to reset the range; helps when there&#39;s large holes between the VMAs.</p></li><li><p><code>tlb_remove_table()</code></p><p><code>tlb_remove_table()</code> is the basic primitive to free page-table directories (<code>__p*_free_tlb()</code>). In it&#39;s most primitive form it is an alias for <code>tlb_remove_page()</code> below, for when page directories are pages and have no additional constraints.</p><p>See also <code>MMU_GATHER_TABLE_FREE</code> and <code>MMU_GATHER_RCU_TABLE_FREE</code>.</p></li><li><p><code>tlb_remove_page() / __tlb_remove_page()</code></p></li><li><p><code>tlb_remove_page_size() / __tlb_remove_page_size()</code></p></li><li><p><code>__tlb_remove_folio_pages()</code></p><p><code>__tlb_remove_page_size()</code> is the basic primitive that queues a page for freeing. <code>__tlb_remove_page()</code> assumes PAGE_SIZE. Both will return a boolean indicating if the queue is (now) full and a call to <code>tlb_flush_mmu()</code> is required.</p><p><code>tlb_remove_page()</code> and <code>tlb_remove_page_size()</code> imply the call to <code>tlb_flush_mmu()</code> when required and has no return value.</p><p><code>__tlb_remove_folio_pages()</code> is similar to <code>__tlb_remove_page()</code>, however, instead of removing a single page, remove the given number of consecutive pages that are all part of the same (large) folio: just like calling <code>__tlb_remove_page()</code> on each page individually.</p></li><li><p><code>tlb_change_page_size()</code></p><p>call before <code>__tlb_remove_page*()</code> to set the current page-size; implies a possible <code>tlb_flush_mmu()</code> call.</p></li><li><p><code>tlb_flush_mmu() / tlb_flush_mmu_tlbonly()</code></p><p><code>tlb_flush_mmu_tlbonly()</code> - does the TLB invalidate (and resets related state, like the range)</p><p><code>tlb_flush_mmu()</code> - in addition to the above TLB invalidate, also frees whatever pages are still batched.</p></li><li><p><code>mmu_gather::fullmm</code></p><p>A flag set by <code>tlb_gather_mmu_fullmm()</code> to indicate we&#39;re going to free the entire mm; this allows a number of optimizations.</p><ul><li><p>We can ignore <code>tlb_{start,end}_vma()</code>; because we don&#39;t care about ranges. Everything will be shot down.</p></li><li><p>(RISC) architectures that use ASIDs can cycle to a new ASID and delay the invalidation until ASID space runs out.</p></li></ul></li><li><p><code>mmu_gather::need_flush_all</code></p><p>A flag that can be set by the arch code if it wants to force flush the entire TLB irrespective of the range. For instance x86-PAE needs this when changing top-level entries.</p></li></ul><p>And allows the architecture to provide and implement <code>tlb_flush()</code>:</p><p><code>tlb_flush()</code> may, in addition to the above mentioned mmu_gather fields, make use of:</p><ul><li><p><code>mmu_gather::start / mmu_gather::end</code></p><p>which provides the range that needs to be flushed to cover the pages to be freed.</p></li><li><p><code>mmu_gather::freed_tables</code></p><p>set when we freed page table pages</p></li><li><p><code>tlb_get_unmap_shift() / tlb_get_unmap_size()</code></p><p>returns the smallest TLB entry size unmapped in this range.</p></li></ul><p>If an architecture does not provide <code>tlb_flush()</code> a default implementation based on <code>flush_tlb_range()</code> will be used, unless MMU_GATHER_NO_RANGE is specified, in which case we&#39;ll default to <code>flush_tlb_mm()</code>.</p><p>Additionally there are a few opt-in features:</p><ul><li><p><code>MMU_GATHER_PAGE_SIZE</code></p><p>This ensures we call <code>tlb_flush()</code> every time <code>tlb_change_page_size()</code> actually changes the size and provides mmu_gather::page_size to <code>tlb_flush()</code>.</p><p>This might be useful if your architecture has size specific TLB invalidation instructions.</p></li><li><p><code>MMU_GATHER_TABLE_FREE</code></p><p>This provides tlb_remove_table(), to be used instead of <code>tlb_remove_page()</code> for page directores (<code>__p*_free_tlb()</code>).</p><p>Useful if your architecture has non-page page directories.</p><p>When used, an architecture is expected to provide <code>__tlb_remove_table()</code> which does the actual freeing of these pages.</p></li><li><p><code>MMU_GATHER_RCU_TABLE_FREE</code></p><p>Like <code>MMU_GATHER_TABLE_FREE</code>, and adds semi-RCU semantics to the free (see comment below).</p><p>Useful if your architecture doesn&#39;t use IPIs for remote TLB invalidates and therefore doesn&#39;t naturally serialize with software page-table walkers.</p></li><li><p><code>MMU_GATHER_NO_FLUSH_CACHE</code></p><p>Indicates the architecture has <code>flush_cache_range()</code> but it needs <em>NOT</em> be called before unmapping a VMA.</p><p>NOTE: strictly speaking we shouldn&#39;t have this knob and instead rely on <code>flush_cache_range()</code> being a NOP, except Sparc64 seems to be different here.</p></li><li><p><code>MMU_GATHER_MERGE_VMAS</code></p><p>Indicates the architecture wants to merge ranges over VMAs; typical when multiple range invalidates are more expensive than a full invalidate.</p></li><li><p><code>MMU_GATHER_NO_RANGE</code></p><p>Use this if your architecture lacks an efficient flush_tlb_range(). This option implies <code>MMU_GATHER_MERGE_VMAS</code> above.</p></li><li><p><code>MMU_GATHER_NO_GATHER</code></p><p>If the option is set the mmu_gather will not track individual pages for delayed page free anymore. A platform that enables the option needs to provide its own implementation of the <code>__tlb_remove_page_size()</code> function to free pages.</p><p>This is useful if your architecture already flushes TLB entries in the various <code>ptep_get_and_clear()</code> functions.</p></li></ul>`,25)]))}const m=a(l,[["render",n]]);export{_ as __pageData,m as default};
